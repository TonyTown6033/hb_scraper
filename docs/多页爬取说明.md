# 多页爬取功能说明

## 🌟 功能概述

多页爬取功能允许你自动爬取网站的所有分页，获取完整的产品数据，而不仅仅是第一页的内容。

## ✨ 主要特性

### 1. 智能翻页
- 自动检测下一页按钮
- 自动点击并等待页面加载
- 检测是否已到最后一页

### 2. 断点续传
- 自动保存爬取进度到 `data/output/scrape_progress.json`
- 中断后可继续之前的爬取
- 避免重复爬取已完成的页面

### 3. 灵活的爬取模式
- **单页模式** - 仅爬取第一页（快速测试）
- **多页模式** - 爬取所有页面（完整数据）
- **限制页数** - 爬取指定数量的页面

## 📖 使用方法

### 方法 1: 交互式运行（推荐新手）

```bash
uv run python main.py
```

运行后会提示选择：
```
爬取模式:
  1. 单页模式 - 仅爬取第一页（快速测试）
  2. 多页模式 - 爬取所有页面（完整数据）
  3. 限制页数 - 爬取指定页数

选择模式 (1/2/3, 默认1):
```

### 方法 2: 非交互式运行（适合自动化）

```bash
# 爬取所有页面
uv run python scripts/scrape_multi_pages.py

# 只爬取前 3 页
uv run python scripts/scrape_multi_pages.py --max-pages 3

# 指定其他分类URL
uv run python scripts/scrape_multi_pages.py --url "https://www.hollandandbarrett.com/shop/..."
```

### 方法 3: Python 代码中使用

```python
from utils.multi_page_scraper import scrape_all_pages

# 爬取所有页面
products = scrape_all_pages(
    driver=driver,
    base_url="https://...",
    scrape_single_page_func=your_scrape_function,
    max_pages=None,  # None = 不限制
    enable_resume=True  # 启用断点续传
)

# 限制爬取 5 页
products = scrape_all_pages(
    driver=driver,
    base_url="https://...",
    scrape_single_page_func=your_scrape_function,
    max_pages=5,
    enable_resume=True
)
```

## 🔄 断点续传功能

### 工作原理

1. **自动保存进度**: 每爬取完一页，进度自动保存到 `data/output/scrape_progress.json`
2. **检测中断**: 下次运行时自动检测是否有未完成的任务
3. **询问恢复**: 提示用户是否继续之前的爬取
4. **继续爬取**: 从上次中断的页面继续

### 进度文件示例

```json
{
  "base_url": "https://www.hollandandbarrett.com/shop/...",
  "last_page": 3,
  "pages_scraped": 3,
  "total_products": 60,
  "products": [...],
  "timestamp": "2025-11-15 18:00:00"
}
```

### 使用场景

- **网络中断**: 网络不稳定时，可以随时中断后继续
- **测试调试**: 先爬几页测试，确认无误后继续
- **分批爬取**: 今天爬一部分，明天继续
- **资源限制**: CPU/内存不足时，可以分批处理

### 注意事项

⚠️ **何时会清除进度**：
- 正常完成所有页面的爬取
- 手动删除 `data/output/scrape_progress.json`
- 更换爬取的URL

⚠️ **何时不保存进度**：
- 设置 `enable_resume=False`
- 测试模式运行

## 📊 爬取过程示例

```
======================================================================
开始多页爬取
======================================================================
起始页: 1
最大页数: 不限制
======================================================================

======================================================================
正在爬取第 1 页
======================================================================

正在访问: https://www.hollandandbarrett.com/shop/vitamins-supplements/...
✓ 已接受 Cookie
✓ 产品列表已加载
✓ 找到 20 个产品
  [1] Mushrooms For Life - Organic Tremella Pure Grade Extract...
  [2] Vitawell - Hair Lush Locks Gummies 60s...
  ...
✓ 第 1 页爬取完成，获得 20 个产品
✓ 累计: 20 个产品

→ 点击下一页按钮...
✓ 成功跳转到第 2 页

======================================================================
正在爬取第 2 页
======================================================================
...
```

## ⚡ 性能优化建议

### 1. 控制爬取速度
- 每页之间自动延迟 2 秒
- 避免触发反爬虫机制

### 2. 使用限制页数
```bash
# 先测试前 2 页
uv run python scripts/scrape_multi_pages.py --max-pages 2

# 确认无误后爬取所有页
uv run python scripts/scrape_multi_pages.py
```

### 3. 分批处理数据
- 基础信息爬取：多页爬取所有产品URL
- 详情信息爬取：分批处理产品详情
- 图片处理：最后统一处理图片

## 🐛 常见问题

### Q1: 如何知道一共有多少页？

运行后会自动检测并显示总页数（如果网站提供）。

### Q2: 爬取过程中被中断了怎么办？

直接重新运行，程序会询问是否继续之前的进度。

### Q3: 如何清除进度重新开始？

删除文件：`data/output/scrape_progress.json`

### Q4: 支持哪些网站的分页？

目前针对 Holland & Barrett 网站优化，理论上支持类似结构的电商网站。

### Q5: 如何修改延迟时间？

编辑 `utils/multi_page_scraper.py`，找到 `time.sleep(2)` 修改数值。

## 📝 完整示例

```bash
# 1. 先测试单页
uv run python main.py
# 选择: 1 (单页模式)

# 2. 确认无误后，爬取前 3 页测试
uv run python scripts/scrape_multi_pages.py --max-pages 3

# 3. 最后爬取所有页面
uv run python scripts/scrape_multi_pages.py

# 4. 输出文件位于:
# data/output/products_multi_page_vitamins-supplements.csv
```

## 🎯 最佳实践

1. **先小后大**: 先测试 1-2 页，确认无误后再爬取全部
2. **定期备份**: 重要数据及时备份到其他位置
3. **监控进度**: 留意控制台输出，发现异常及时中断
4. **合理延迟**: 不要设置过短的延迟时间
5. **分时爬取**: 避开网站高峰期进行大规模爬取

## 🔗 相关文档

- [快速启动指南](../QUICKSTART.md)
- [项目结构说明](../PROJECT_STRUCTURE.md)
- [主 README](../README.md)

# 更新总结 - 失败记录与重爬功能

## 🎯 本次更新内容

### 1. 删除极速模式 ❌

**原因**: 极速模式（8-20个线程）容易被限流，成功率低，不太实用。

**变更**:
- 从 `main.py` 中移除极速模式选项
- 只保留**顺序模式**和**并行模式**（3-10个线程）
- 更新所有相关文档

### 2. 新增失败产品自动记录 ⭐

**功能**: 爬取过程中失败的产品会自动记录

**记录位置**:
```
data/output/failed_products.json
```

**记录内容**:
```json
{
  "item_data": { "name": "...", "url": "...", ... },
  "error": "失败原因",
  "timestamp": "2024-11-18T12:30:45",
  "url": "产品URL"
}
```

**支持的模式**:
- ✅ 顺序模式
- ✅ 并行模式

### 3. 新增失败产品重爬脚本 ⭐

**脚本路径**:
```
scripts/retry_failed.py
```

**使用方法**:
```bash
uv run python scripts/retry_failed.py
```

**功能特性**:
- 📊 显示失败产品列表和统计
- 📈 按时间和错误类型分类
- 🔄 使用更保守的配置重爬（5次重试，3-6秒延迟）
- 💾 自动更新失败记录
- 📁 可选保存成功结果到CSV

**工作流程**:
```
1. 读取 failed_products.json
2. 显示失败摘要（数量、原因、时间）
3. 询问是否重爬
4. 使用保守配置重新爬取
5. 显示重爬结果（成功/失败）
6. 更新失败记录（只保留仍失败的）
7. 可选保存到 retry_success.csv
```

## 📦 新增文件

### 核心功能
- `utils/parallel_scraper.py` - 添加失败记录功能（修改）
- `scripts/retry_failed.py` - 失败产品重爬工具（新增）

### 文档
- `docs/RETRY_FAILED.md` - 失败重爬完整指南（新增）
- `docs/UPDATE_SUMMARY.md` - 本文档（新增）
- `docs/QUICK_START_PARALLEL.md` - 更新，删除极速模式（修改）
- `README.md` - 更新快速开始和文档链接（修改）

## 🔧 代码变更

### main.py

**变更前**:
```python
print("  1. 顺序模式")
print("  2. 并行模式")
print("  3. 极速模式")
```

**变更后**:
```python
print("  1. 顺序模式")
print("  2. 并行模式")
# 极速模式已删除
```

**新增功能**:
- 顺序模式添加失败记录
- 失败后提示使用 `retry_failed.py`

### utils/parallel_scraper.py

**新增**:
```python
self.failed_items = []  # 记录失败的产品

def _save_failed_items(self):
    """保存失败的产品记录"""
    # 合并现有失败记录
    # 保存到 failed_products.json
```

## 📊 使用示例

### 场景1: 正常爬取

```bash
# 1. 运行爬虫
uv run python main.py

# 2. 选择并行模式
选择模式: 2
线程数: 5

# 3. 爬取完成，显示结果
成功: 85/100
失败: 15/100

✗ 15 个失败产品已记录到: data/output/failed_products.json
  可使用 'uv run python scripts/retry_failed.py' 重新爬取
```

### 场景2: 重爬失败产品

```bash
# 1. 运行重爬工具
uv run python scripts/retry_failed.py

# 2. 查看失败摘要
失败产品列表 (共 15 个)
- 网络连接失败: 8 个
- 页面数据提取失败: 7 个

# 3. 确认重爬
是否重新爬取？y
线程数: 2

# 4. 查看结果
重爬结果:
  - 成功: 12/15
  - 仍失败: 3/15

# 5. 保存成功结果
是否保存到CSV？y
✓ 成功产品已保存到: retry_success.csv
```

### 场景3: 多次重试

```bash
# 第一次重爬
uv run python scripts/retry_failed.py
# 成功: 10/15, 仍失败: 5

# 第二次重爬（对剩余5个）
uv run python scripts/retry_failed.py
# 成功: 3/5, 仍失败: 2

# 第三次重爬（对最后2个）
uv run python scripts/retry_failed.py
# 成功: 2/2
🎉 所有产品都已成功爬取！
```

## 🎁 用户收益

### 1. 不再丢失数据 ✅
- 所有失败产品自动记录
- 可随时重爬
- 支持多次重试

### 2. 更高的完整性 ✅
- 通过重爬，最终成功率可达95%+
- 自动统计失败原因
- 方便定位问题

### 3. 更好的体验 ✅
- 一键重爬，简单方便
- 智能配置，提高成功率
- 清晰的进度和统计

### 4. 更灵活的控制 ✅
- 可以选择性重爬
- 可以手动编辑失败列表
- 支持分批处理

## 📈 预期效果

### 爬取100个产品

**首次爬取**（并行模式，5线程）:
- 成功: 85个
- 失败: 15个
- 耗时: 45秒

**失败重爬**（2线程，保守配置）:
- 成功: 12个
- 仍失败: 3个
- 耗时: 30秒

**再次重爬**（1线程，最保守）:
- 成功: 2个
- 仍失败: 1个
- 耗时: 10秒

**总结**:
- 最终成功: 99/100 (99%)
- 总耗时: 85秒
- 比顺序模式快 57%

## 🔜 未来可能改进

- [ ] 支持自动重爬（无需手动运行脚本）
- [ ] 失败产品优先级排序
- [ ] 按失败原因分类重爬
- [ ] 导出失败报告
- [ ] 邮件通知失败产品

## 📚 相关文档

- [失败产品重爬指南](RETRY_FAILED.md) - 详细使用说明
- [多线程并发快速入门](QUICK_START_PARALLEL.md) - 并发爬取指南
- [README](../README.md) - 项目主文档

---

**更新时间**: 2024-11-18
**版本**: v2.1

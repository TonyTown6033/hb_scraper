# 多线程并发爬取 - 快速开始

## 核心概念 🎯

每个线程 = 1个独立的Chrome浏览器实例

```
3个线程 = 3个Chrome同时运行 = 3倍速度
5个线程 = 5个Chrome同时运行 = 5倍速度
```

## 快速使用 ⚡

### 1. 运行爬虫

```bash
uv run python main.py
```

### 2. 选择极速模式

```
爬取模式:
  1. 顺序模式 - 一个接一个爬取（较慢但稳定）
  2. 并行模式 - 多线程同时爬取（快速但消耗更多资源）
  3. 极速模式 - 更多线程并发（最快但可能不稳定）
选择模式 (1/2/3, 默认2): 3  ← 输入3

⚡ 极速模式 - 使用更多WebDriver并发
并发线程数 (建议5-10, 默认8): 10  ← 输入10启动10个Chrome!
```

## 三种模式对比 📊

### 模式1: 顺序模式（稳定但慢）
```
线程数: 1
WebDriver: 1个
速度: ~2秒/产品
成功率: 95%+
适合: 调试、小批量
```

### 模式2: 并行模式（推荐）
```
线程数: 3-5
WebDriver: 3-5个同时运行
速度: ~0.7秒/产品
成功率: 85-90%
适合: 日常使用、中等批量
```

### 模式3: 极速模式（最快）
```
线程数: 5-10
WebDriver: 5-10个同时运行
速度: ~0.3秒/产品
成功率: 70-80%
适合: 大批量、追求速度
```

## 实际效果对比（100个产品）

| 模式 | Chrome数 | 耗时 | 节省时间 |
|------|----------|------|----------|
| 顺序 | 1 | 200秒 | - |
| 并行(3) | 3 | 70秒 | 130秒 (65%) |
| 并行(5) | 5 | 45秒 | 155秒 (77%) |
| 极速(8) | 8 | 30秒 | 170秒 (85%) |
| 极速(10) | 10 | 25秒 | 175秒 (87%) |

## 系统要求 💻

### 根据内存选择线程数

- **4GB 内存**: 最多 2-3 个线程
- **8GB 内存**: 最多 5-8 个线程
- **16GB 内存**: 最多 10-15 个线程
- **32GB 内存**: 可以用 20+ 个线程

### 每个WebDriver占用
- 内存: ~250MB
- CPU: ~5-10%

## 自动找最佳配置 🔍

运行性能测试工具：

```bash
uv run python scripts/find_optimal_threads.py
```

这个工具会：
1. 自动测试 2, 3, 5, 8 个线程
2. 记录每个配置的速度和成功率
3. 推荐最适合你系统的配置

示例输出：
```
线程数   总耗时    平均速度      成功率    吞吐量
2        45.2      4.52         95.0%     0.22
3        32.1      3.21         90.0%     0.31
5        21.5      2.15         85.0%     0.46
8        16.8      1.68         78.0%     0.60

🏆 最佳配置（速度优先，成功率>=80%）:
   线程数: 5
   平均速度: 2.15秒/产品
   成功率: 85.0%
```

## 常见问题 ❓

### Q: 开太多Chrome会不会卡死？
A: 会的！建议根据你的内存来：
- 8GB内存: 不超过8个线程
- 16GB内存: 不超过15个线程

### Q: 为什么10个线程不是10倍速度？
A: 因为：
1. 网络延迟（每个请求2-4秒）
2. 创建WebDriver需要时间
3. 目标网站限流

实际加速比通常是线程数的 60-80%。

### Q: 成功率太低怎么办？
A: 三个方法：
1. 减少线程数
2. 增加延迟时间
3. 增加重试次数

### Q: 能用20个甚至更多线程吗？
A: 理论上可以，但：
- 需要大量内存（5-6GB+）
- 容易被目标网站限流/封IP
- 成功率会很低（<60%）
- 不推荐超过15个

## 高级用法 🚀

### 直接在代码中配置

编辑 `main.py`，找到并修改：

```python
# 极速配置（追求速度）
products = scrape_details_parallel(
    products=products,
    scrape_detail_func=scrape_product_detail,
    max_workers=10,          # 10个WebDriver并发
    retry_times=5,           # 5次重试提高成功率
    request_delay=(0.5, 1),  # 缩短延迟
    enable_headless=True     # 无头模式更快
)
```

```python
# 超稳定配置（追求成功率）
products = scrape_details_parallel(
    products=products,
    scrape_detail_func=scrape_product_detail,
    max_workers=2,           # 只用2个WebDriver
    retry_times=5,           # 5次重试
    request_delay=(5, 8),    # 长延迟避免限流
    enable_headless=False    # 有头模式调试
)
```

## 监控建议 📊

### 使用活动监视器（Mac）/ 任务管理器（Windows）

监控以下指标：
- **内存使用**: 不应超过80%
- **CPU使用**: 正常在50-80%
- **Chrome进程数**: 应该等于线程数

如果内存或CPU爆满，立即减少线程数！

## 最佳实践 ✅

1. **从小到大测试**: 先用3个线程测试10个产品
2. **观察成功率**: 如果<80%就减少线程
3. **监控资源**: 用活动监视器查看内存/CPU
4. **分批爬取**: 不要一次爬几百个，分成50个一批
5. **保存进度**: 多页爬取会自动保存进度

## 推荐配置速查表 📝

| 场景 | 线程数 | 重试 | 延迟 |
|------|--------|------|------|
| 🐌 调试测试 | 1 | 3 | 2-4秒 |
| ⚖️ 日常使用 | 3 | 3 | 2-4秒 |
| ⚡ 快速爬取 | 5 | 3 | 1-3秒 |
| 🚀 极速模式 | 8 | 2 | 0.5-1.5秒 |
| 🏎️ 疯狂模式 | 10+ | 5 | 0.5-1秒 |

## 故障排除 🔧

### Chrome进程过多无法启动

```bash
# 杀掉所有Chrome进程
pkill -9 Chrome
pkill -9 chromedriver
```

### 内存不足

减少线程数或重启电脑释放内存

### 被目标网站封禁

1. 减少线程数
2. 增加延迟时间（5-10秒）
3. 等待一段时间再试
4. 考虑使用代理IP

---

**祝你爬取愉快！🎉**

有问题查看 `docs/PARALLEL_SCRAPING.md` 获取更多详情。

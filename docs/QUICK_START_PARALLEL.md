# 多线程并发爬取 - 快速开始

## 核心概念 🎯

每个线程 = 1个独立的Chrome浏览器实例

```
3个线程 = 3个Chrome同时运行 = 3倍速度
5个线程 = 5个Chrome同时运行 = 5倍速度
```

## 快速使用 ⚡

### 1. 运行爬虫

```bash
uv run python main.py
```

### 2. 选择并行模式

```
爬取模式:
  1. 顺序模式 - 一个接一个爬取（较慢但稳定）
  2. 并行模式 - 多线程同时爬取（推荐，3-5个线程）
选择模式 (1/2, 默认2): 2

提示: 建议使用3-5个线程以平衡速度和稳定性
并发线程数 (建议3-5, 默认3): 5  ← 输入5启动5个Chrome

使用并行模式爬取 300 个产品，5 个线程并发
配置: 3次重试, 2-4秒随机延迟
💡 每100个产品自动写入CSV，避免内存占用过大  ← 自动分批写入
```

## 两种模式对比 📊

### 模式1: 顺序模式
```
线程数: 1
Chrome实例: 1个
速度: ~2秒/产品
成功率: 95%+
适合: 调试、小批量(<20个产品)
```

### 模式2: 并行模式（推荐）
```
线程数: 3-5
Chrome实例: 3-5个同时运行
速度: ~0.7秒/产品
成功率: 85-90%
适合: 所有场景
```

## 实际效果对比（100个产品）

| 模式 | Chrome数 | 耗时 | 节省时间 |
|------|----------|------|----------|
| 顺序 | 1 | 200秒 | - |
| 并行(3) | 3 | 70秒 | 130秒 (65%) |
| 并行(5) | 5 | 45秒 | 155秒 (77%) |

## 系统要求 💻

### 根据内存选择线程数

- **4GB 内存**: 最多 2-3 个线程
- **8GB 内存**: 推荐 3-5 个线程 ⭐
- **16GB 内存**: 最多 8-10 个线程

### 每个Chrome占用
- 内存: ~250MB
- CPU: ~5-10%

## 失败产品自动记录 📋

### 自动记录

爬取过程中失败的产品会自动保存到：
```
data/output/failed_products.json
```

每条记录包含：
- 产品基本信息
- 失败原因
- 失败时间

### 重新爬取失败产品

```bash
uv run python scripts/retry_failed.py
```

脚本功能：
- 显示失败产品列表和统计
- 使用更保守的配置重爬（更多重试、更长延迟）
- 自动更新失败记录
- 可选保存成功结果到CSV

**示例输出**：
```
======================================================================
失败产品列表 (共 15 个)
======================================================================

2024-11-18: 12 个失败
2024-11-17: 3 个失败

======================================================================
失败原因统计:
======================================================================

  - 网络连接失败: 8 个
  - 页面数据提取失败: 5 个
  - 超时: 2 个

是否重新爬取这 15 个失败的产品？(y/n): y

重爬结果:
  - 成功: 12/15
  - 仍失败: 3/15

🎉 大部分产品已成功爬取！
```

详细说明见 [失败产品重爬指南](RETRY_FAILED.md)

## 分批写入CSV - 节省内存 📦

### 自动启用

并行模式会**自动分批写入CSV**，无需配置：

**运行示例**（爬取300个产品）:
```
进度: 100/300 (33.3%) - 成功: 90, 失败: 10

📦 批次 1: 已完成 100 个产品，正在写入CSV...
✓ 批次 1 已写入 100 个产品到 data/output/products_complete.csv

进度: 200/300 (66.7%) - 成功: 180, 失败: 20

📦 批次 2: 已完成 100 个产品，正在写入CSV...
✓ 批次 2 已写入 100 个产品到 data/output/products_complete.csv

进度: 300/300 (100.0%) - 成功: 270, 失败: 30

📦 批次 3 (最后一批): 已完成 100 个产品，正在写入CSV...
✓ 批次 3 已写入 100 个产品到 data/output/products_complete.csv
```

### 优势

- **内存稳定**: 最多只保存100个产品在内存中
- **数据安全**: 已完成的批次不会因中断而丢失
- **实时可见**: 可以随时打开CSV查看已爬取的数据
- **大规模支持**: 支持爬取几千个产品

### 详细说明

查看完整文档: [分批写入CSV说明](BATCH_WRITE.md)

## 自动找最佳配置 🔍

运行性能测试工具：

```bash
uv run python scripts/find_optimal_threads.py
```

这个工具会：
1. 自动测试 2, 3, 5, 8 个线程
2. 记录每个配置的速度和成功率
3. 推荐最适合你系统的配置

示例输出：
```
线程数   总耗时    平均速度      成功率    吞吐量
2        45.2      4.52         95.0%     0.22
3        32.1      3.21         90.0%     0.31
5        21.5      2.15         85.0%     0.46

🏆 最佳配置（速度优先，成功率>=80%）:
   线程数: 5
   平均速度: 2.15秒/产品
   成功率: 85.0%
```

## 常见问题 ❓

### Q: 为什么5个线程不是5倍速度？
A: 因为：
1. 网络延迟（每个请求2-4秒）
2. 创建Chrome需要时间
3. 目标网站可能限流

实际加速比通常是线程数的 60-80%。

### Q: 成功率太低怎么办？
A: 三个方法：
1. 减少线程数（降到2-3）
2. 增加延迟时间
3. 使用失败重爬功能

### Q: 失败的产品会丢失吗？
A: **不会！** 所有失败的产品都会自动记录到 `failed_products.json`，可以随时重爬。

### Q: 能用更多线程吗（如10个）？
A: 可以，但：
- 需要更多内存（~2.5GB+）
- 容易被限流
- 成功率会降低
- 建议不超过10个

## 推荐配置 ✅

### 日常使用（推荐）
```
线程数: 3-5
重试: 3次
延迟: 2-4秒
预期成功率: 85-90%
```

### 追求稳定
```
线程数: 2
重试: 3次
延迟: 3-5秒
预期成功率: 95%+
```

## 最佳实践 💡

1. **从小到大测试**: 先用3个线程测试10个产品
2. **观察成功率**: 如果<85%就减少线程
3. **监控资源**: 用活动监视器查看内存/CPU
4. **及时重爬**: 爬取完成后立即运行 `retry_failed.py`
5. **保存进度**: 多页爬取会自动保存进度

## 故障排除 🔧

### Chrome进程过多无法启动

```bash
# 杀掉所有Chrome进程
pkill -9 Chrome
pkill -9 chromedriver
```

### 内存不足

减少线程数或重启电脑释放内存

### 被目标网站封禁

1. 减少线程数到2
2. 增加延迟时间（5-10秒）
3. 等待一段时间再试

### 失败率过高（>20%）

1. 使用 `retry_failed.py` 重爬失败产品
2. 减少线程数
3. 增加延迟和重试次数

---

**有问题？** 查看完整文档：
- [失败产品重爬指南](RETRY_FAILED.md)
- [并行爬取详解](PARALLEL_SCRAPING.md)
- [WebDriver架构说明](WEBDRIVER_ARCHITECTURE.md)

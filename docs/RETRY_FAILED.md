# 失败产品重爬指南 📋

## 概述

爬取过程中，部分产品可能因为网络问题、页面加载慢、限流等原因导致失败。本工具会自动记录所有失败的产品，并提供便捷的重爬功能。

## 工作原理

### 1. 自动记录失败

在爬取过程中，无论是顺序模式还是并行模式，所有失败的产品都会自动记录到：

```
data/output/failed_products.json
```

### 2. 失败记录格式

```json
[
  {
    "item_data": {
      "name": "Product Name",
      "url": "https://...",
      "brand": "Brand",
      "price": "£10.99",
      "image": "https://..."
    },
    "error": "Could not reach host. Are you offline?",
    "timestamp": "2024-11-18T12:30:45",
    "url": "https://..."
  }
]
```

每条记录包含：
- **item_data**: 产品的基本信息（来自列表页）
- **error**: 失败原因
- **timestamp**: 失败时间
- **url**: 产品详情页URL

### 3. 何时会记录失败

以下情况会被记录为失败：

#### 并行模式
- 所有重试都失败（默认3次）
- 网络连接失败
- 页面超时
- 数据提取失败

#### 顺序模式
- 抛出异常
- 未获取到详情数据

## 使用重爬工具

### 基本使用

```bash
uv run python scripts/retry_failed.py
```

### 工作流程

#### 1. 查看失败摘要

脚本会自动显示：

```
======================================================================
失败产品列表 (共 15 个)
======================================================================

2024-11-18: 12 个失败
2024-11-17: 3 个失败

======================================================================
失败原因统计:
======================================================================

  - 网络连接失败: 8 个
  - 页面数据提取失败: 5 个
  - 超时: 2 个
```

#### 2. 确认重爬

```
是否重新爬取这 15 个失败的产品？(y/n): y
```

#### 3. 选择线程数

```
提示: 建议使用较少的线程数和较长的延迟来提高成功率
并发线程数 (建议2-3, 默认2): 2
```

#### 4. 自动重爬

脚本使用更保守的配置：
- **重试次数**: 5次（比正常爬取多）
- **延迟时间**: 3-6秒（比正常爬取长）
- **线程数**: 默认2个（比正常爬取少）

#### 5. 查看结果

```
重爬结果:
  - 成功: 12/15
  - 仍失败: 3/15
```

#### 6. 更新失败记录

- **全部成功**: 自动删除 `failed_products.json`
- **部分成功**: 只保留仍然失败的产品

#### 7. 保存成功结果（可选）

```
是否将成功爬取的产品保存到CSV？(y/n): y
✓ 成功产品已保存到: data/output/retry_success.csv
```

## 常见失败原因

### 1. 网络连接失败

**错误信息**:
```
Could not reach host. Are you offline?
```

**原因**:
- 并发请求过多被限流
- 网络不稳定
- 目标服务器暂时不可用

**解决方案**:
```bash
# 减少线程数，增加延迟
并发线程数: 2
重试次数: 5次
延迟: 3-6秒
```

### 2. 页面数据提取失败

**错误信息**:
```
未找到__LAYOUT__数据
```

**原因**:
- 页面加载不完整
- JavaScript未执行完成
- 页面结构变化

**解决方案**:
- 使用顺序模式重爬
- 手动检查单个失败产品的URL
- 查看页面源代码确认结构

### 3. 超时

**错误信息**:
```
timeout
```

**原因**:
- 网络慢
- 页面加载慢
- 服务器响应慢

**解决方案**:
- 增加页面加载超时时间
- 使用更少的线程
- 在网络好的时候重试

## 高级用法

### 查看失败记录

```bash
# 直接查看JSON文件
cat data/output/failed_products.json | python -m json.tool
```

### 手动编辑失败记录

如果某些产品确定不需要重爬，可以手动编辑JSON文件删除：

```bash
# 使用文本编辑器
code data/output/failed_products.json
```

### 清空失败记录

```bash
# 删除失败记录文件
rm data/output/failed_products.json
```

### 分批重爬

如果失败产品很多（如100+），建议分批重爬：

1. 备份失败记录：
```bash
cp data/output/failed_products.json data/output/failed_products_backup.json
```

2. 手动编辑 `failed_products.json`，只保留前30个

3. 运行重爬：
```bash
uv run python scripts/retry_failed.py
```

4. 重复步骤2-3，直到全部重爬完成

## 最佳实践

### 1. 及时重爬

建议在爬取完成后立即查看失败记录：

```bash
# 爬取完成后
uv run python main.py

# 如果有失败，立即重爬
uv run python scripts/retry_failed.py
```

### 2. 使用保守配置

重爬时建议：
- 线程数: 2-3（不要太多）
- 延迟: 3-6秒（更长一些）
- 重试: 5次（更多一些）

### 3. 多次重试

如果第一次重爬后仍有失败，可以再次运行：

```bash
# 第一次重爬
uv run python scripts/retry_failed.py

# 查看仍然失败的数量
cat data/output/failed_products.json

# 如果还有失败，再次重爬
uv run python scripts/retry_failed.py
```

### 4. 检查失败原因

如果某个产品反复失败，建议：

1. 手动访问该URL
2. 检查页面是否正常
3. 查看是否需要登录
4. 确认页面结构是否变化

### 5. 合并结果

重爬成功后，需要手动合并到主CSV：

```bash
# 将 retry_success.csv 的内容追加到 products_complete.csv
# 可以使用Excel或脚本合并
```

## 故障排除

### 问题1: 重爬后全部仍然失败

**可能原因**:
- 网络问题
- IP被封
- 网站结构变化

**解决方案**:
1. 检查网络连接
2. 等待一段时间（1-2小时）
3. 使用顺序模式重试
4. 检查是否需要更新爬虫代码

### 问题2: 找不到failed_products.json

**原因**: 没有失败的产品

**解决方案**: 这是好事！说明所有产品都成功了。

### 问题3: 重爬时内存不足

**原因**: 失败产品太多，并发太高

**解决方案**:
1. 减少线程数到1-2
2. 分批重爬（见上文）

### 问题4: 保存的CSV找不到数据

**原因**: 可能使用了错误的字段名

**解决方案**: 检查 `retry_success.csv`，确认字段映射正确

## 统计和监控

### 查看失败统计

```python
import json

with open('data/output/failed_products.json', 'r', encoding='utf-8') as f:
    failed = json.load(f)

print(f"总失败数: {len(failed)}")

# 按错误类型统计
from collections import Counter
errors = [item['error'][:50] for item in failed]
print(Counter(errors))
```

### 失败率分析

```python
# 假设爬取了100个产品，失败15个
success_rate = (100 - 15) / 100 * 100
print(f"成功率: {success_rate}%")

# 如果成功率 < 80%，需要优化配置
if success_rate < 80:
    print("建议: 减少线程数，增加延迟")
```

## 总结

失败重爬功能的优势：

✅ **自动记录** - 无需手动记录失败产品
✅ **详细信息** - 记录失败原因和时间
✅ **智能重试** - 使用更保守的配置
✅ **增量更新** - 只保留仍然失败的记录
✅ **灵活导出** - 可单独保存成功结果

通过这个功能，你可以确保不遗漏任何产品，提高爬取的完整性！

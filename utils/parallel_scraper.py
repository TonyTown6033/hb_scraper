"""å¹¶è¡Œçˆ¬å–å·¥å…· - ä½¿ç”¨å¤šçº¿ç¨‹åŠ é€Ÿè¯¦æƒ…é¡µçˆ¬å–"""

import time
import random
import json
from pathlib import Path
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock, Semaphore
from typing import List, Dict, Callable, Any
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from utils.logger import get_logger


class ParallelScraper:
    """å¹¶è¡Œçˆ¬å–ç®¡ç†å™¨"""

    def __init__(
        self,
        max_workers: int = 4,
        retry_times: int = 3,
        request_delay: tuple = (1, 3),
        enable_headless: bool = True
    ):
        """
        åˆå§‹åŒ–å¹¶è¡Œçˆ¬å–å™¨

        Args:
            max_workers: æœ€å¤§å¹¶å‘çº¿ç¨‹æ•°ï¼Œé»˜è®¤4ä¸ª
            retry_times: å¤±è´¥é‡è¯•æ¬¡æ•°ï¼Œé»˜è®¤3æ¬¡
            request_delay: è¯·æ±‚å»¶è¿ŸèŒƒå›´(æœ€å°, æœ€å¤§)ç§’ï¼Œé»˜è®¤(1, 3)
            enable_headless: æ˜¯å¦å¯ç”¨æ— å¤´æ¨¡å¼ï¼Œé»˜è®¤True
        """
        self.max_workers = max_workers
        self.retry_times = retry_times
        self.request_delay = request_delay
        self.enable_headless = enable_headless
        self.logger = get_logger()
        self.lock = Lock()  # ç”¨äºä¿æŠ¤å…±äº«èµ„æº
        self.rate_limiter = Semaphore(max_workers)  # é™æµæ§åˆ¶
        self.failed_items = []  # è®°å½•å¤±è´¥çš„äº§å“

    def _create_driver(self) -> webdriver.Chrome:
        """
        ä¸ºæ¯ä¸ªçº¿ç¨‹åˆ›å»ºç‹¬ç«‹çš„WebDriverå®ä¾‹

        Returns:
            webdriver.Chrome: Chrome WebDriverå®ä¾‹
        """
        options = webdriver.ChromeOptions()
        options.add_argument("--disable-blink-features=AutomationControlled")
        options.add_experimental_option("excludeSwitches", ["enable-automation"])
        options.add_experimental_option("useAutomationExtension", False)

        if self.enable_headless:
            options.add_argument("--headless=new")  # æ–°ç‰ˆæ— å¤´æ¨¡å¼

        # æ€§èƒ½ä¼˜åŒ–é€‰é¡¹
        options.add_argument("--disable-gpu")
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-dev-shm-usage")
        options.add_argument("--disable-extensions")
        options.add_argument("--disable-images")  # ç¦ç”¨å›¾ç‰‡åŠ è½½ï¼Œæé€Ÿ

        # è®¾ç½®User-Agentï¼Œé¿å…è¢«è¯†åˆ«ä¸ºçˆ¬è™«
        options.add_argument(
            "user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
            "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        )

        # è®¾ç½®é¡µé¢åŠ è½½ç­–ç•¥
        options.page_load_strategy = 'normal'

        service = Service(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=options)

        # è®¾ç½®è¶…æ—¶æ—¶é—´
        driver.set_page_load_timeout(30)  # é¡µé¢åŠ è½½è¶…æ—¶30ç§’
        driver.set_script_timeout(30)  # è„šæœ¬æ‰§è¡Œè¶…æ—¶30ç§’

        return driver

    def _scrape_single_item(
        self,
        item_data: Dict,
        scrape_func: Callable,
        item_index: int,
        total_items: int
    ) -> Dict:
        """
        çˆ¬å–å•ä¸ªé¡¹ç›®ï¼ˆåœ¨å•ç‹¬çš„çº¿ç¨‹ä¸­è¿è¡Œï¼Œå¸¦é‡è¯•æœºåˆ¶ï¼‰

        Args:
            item_data: é¡¹ç›®æ•°æ®
            scrape_func: çˆ¬å–å‡½æ•°
            item_index: å½“å‰ç´¢å¼•
            total_items: æ€»æ•°

        Returns:
            Dict: æ›´æ–°åçš„é¡¹ç›®æ•°æ®
        """
        driver = None
        url = item_data.get("url", "")

        # ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘è¯·æ±‚é€Ÿç‡
        with self.rate_limiter:
            # æ·»åŠ éšæœºå»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡å¿«
            delay = random.uniform(*self.request_delay)
            time.sleep(delay)

            # é‡è¯•æœºåˆ¶
            for attempt in range(1, self.retry_times + 1):
                try:
                    # å¦‚æœä¸æ˜¯ç¬¬ä¸€æ¬¡å°è¯•ï¼Œå…ˆå…³é—­ä¹‹å‰çš„driver
                    if driver:
                        try:
                            driver.quit()
                        except:
                            pass
                        time.sleep(2)  # é‡è¯•å‰ç­‰å¾…

                    # åˆ›å»ºæ–°çš„driver
                    driver = self._create_driver()

                    # æ‰§è¡Œçˆ¬å–
                    if attempt > 1:
                        self.logger.warning(
                            f"[{item_index}/{total_items}] é‡è¯• {attempt}/{self.retry_times}: {url}"
                        )
                    else:
                        self.logger.info(f"[{item_index}/{total_items}] å¼€å§‹çˆ¬å–: {url}")

                    details = scrape_func(driver, url)

                    # åˆå¹¶æ•°æ®
                    result = {**item_data, **details}

                    self.logger.info(
                        f"[{item_index}/{total_items}] âœ“ å®Œæˆ: {item_data.get('name', 'Unknown')[:40]}"
                    )

                    return result

                except Exception as e:
                    error_msg = str(e)
                    if attempt < self.retry_times:
                        self.logger.warning(
                            f"[{item_index}/{total_items}] å¤±è´¥ (å°è¯• {attempt}/{self.retry_times}): "
                            f"{error_msg[:100]}"
                        )
                    else:
                        self.logger.error(
                            f"[{item_index}/{total_items}] âœ— æœ€ç»ˆå¤±è´¥: {error_msg[:100]}"
                        )
                        # è®°å½•å¤±è´¥ä¿¡æ¯
                        with self.lock:
                            self.failed_items.append({
                                "item_data": item_data,
                                "error": error_msg[:200],
                                "timestamp": datetime.now().isoformat(),
                                "url": url
                            })

                finally:
                    # ç¡®ä¿driverè¢«å…³é—­
                    if driver and attempt == self.retry_times:
                        try:
                            driver.quit()
                        except:
                            pass

            # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥ï¼Œè¿”å›åŸå§‹æ•°æ®
            return item_data

    def scrape_items_parallel(
        self,
        items: List[Dict],
        scrape_func: Callable,
        max_items: int = None,
        batch_size: int = None,
        batch_callback: Callable = None
    ) -> List[Dict]:
        """
        å¹¶è¡Œçˆ¬å–å¤šä¸ªé¡¹ç›®

        Args:
            items: è¦çˆ¬å–çš„é¡¹ç›®åˆ—è¡¨
            scrape_func: å•ä¸ªé¡¹ç›®çš„çˆ¬å–å‡½æ•°ï¼Œæ¥æ”¶(driver, url)å‚æ•°
            max_items: æœ€å¤§çˆ¬å–æ•°é‡ï¼ŒNoneè¡¨ç¤ºå…¨éƒ¨
            batch_size: åˆ†æ‰¹å¤§å°ï¼Œæ¯çˆ¬å–Nä¸ªå°±è°ƒç”¨å›è°ƒå‡½æ•°ï¼ŒNoneè¡¨ç¤ºä¸åˆ†æ‰¹
            batch_callback: åˆ†æ‰¹å›è°ƒå‡½æ•°ï¼Œæ¥æ”¶(batch_results, batch_num)å‚æ•°

        Returns:
            List[Dict]: çˆ¬å–åçš„æ•°æ®åˆ—è¡¨
        """
        # ç¡®å®šè¦çˆ¬å–çš„æ•°é‡
        items_to_scrape = items[:max_items] if max_items else items
        total_items = len(items_to_scrape)

        self.logger.info(
            f"å¼€å§‹å¹¶è¡Œçˆ¬å– {total_items} ä¸ªé¡¹ç›®\n"
            f"  - çº¿ç¨‹æ•°: {self.max_workers}\n"
            f"  - é‡è¯•æ¬¡æ•°: {self.retry_times}\n"
            f"  - è¯·æ±‚å»¶è¿Ÿ: {self.request_delay[0]}-{self.request_delay[1]}ç§’"
        )

        start_time = time.time()
        results = []
        batch_results = []  # ä¸´æ—¶æ‰¹æ¬¡ç»“æœ
        completed_count = 0
        success_count = 0
        failed_count = 0
        batch_num = 0  # å½“å‰æ‰¹æ¬¡å·

        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œæ‰§è¡Œ
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_item = {
                executor.submit(
                    self._scrape_single_item,
                    item,
                    scrape_func,
                    idx + 1,
                    total_items
                ): item
                for idx, item in enumerate(items_to_scrape)
            }

            # æ”¶é›†ç»“æœ
            for future in as_completed(future_to_item):
                try:
                    result = future.result()
                    results.append(result)
                    batch_results.append(result)  # æ·»åŠ åˆ°æ‰¹æ¬¡ç»“æœ
                    completed_count += 1

                    # æ£€æŸ¥æ˜¯å¦æˆåŠŸçˆ¬å–åˆ°è¯¦æƒ…
                    original_item = future_to_item[future]
                    if result != original_item and any(
                        key in result for key in ['highlights', 'description', 'directions']
                    ):
                        success_count += 1
                    else:
                        failed_count += 1

                    # æ£€æŸ¥æ˜¯å¦éœ€è¦æ‰§è¡Œæ‰¹æ¬¡å›è°ƒ
                    if batch_size and batch_callback and len(batch_results) >= batch_size:
                        batch_num += 1
                        self.logger.info(
                            f"\nğŸ“¦ æ‰¹æ¬¡ {batch_num}: å·²å®Œæˆ {len(batch_results)} ä¸ªäº§å“ï¼Œæ­£åœ¨å†™å…¥CSV..."
                        )
                        batch_callback(batch_results, batch_num)
                        batch_results = []  # æ¸…ç©ºæ‰¹æ¬¡ç»“æœ

                    # æ˜¾ç¤ºè¿›åº¦ï¼ˆæ¯5ä¸ªæ˜¾ç¤ºä¸€æ¬¡ï¼Œé¿å…åˆ·å±ï¼‰
                    if completed_count % 5 == 0 or completed_count == total_items:
                        elapsed = time.time() - start_time
                        avg_time = elapsed / completed_count
                        remaining = (total_items - completed_count) * avg_time

                        self.logger.info(
                            f"è¿›åº¦: {completed_count}/{total_items} "
                            f"({completed_count/total_items*100:.1f}%) - "
                            f"æˆåŠŸ: {success_count}, å¤±è´¥: {failed_count} - "
                            f"å·²ç”¨æ—¶: {elapsed:.1f}s - "
                            f"é¢„è®¡å‰©ä½™: {remaining:.1f}s"
                        )

                except Exception as e:
                    self.logger.error(f"ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}")
                    failed_count += 1

            # å¤„ç†æœ€åä¸€æ‰¹ï¼ˆå¦‚æœæœ‰å‰©ä½™ï¼‰
            if batch_size and batch_callback and batch_results:
                batch_num += 1
                self.logger.info(
                    f"\nğŸ“¦ æ‰¹æ¬¡ {batch_num} (æœ€åä¸€æ‰¹): å·²å®Œæˆ {len(batch_results)} ä¸ªäº§å“ï¼Œæ­£åœ¨å†™å…¥CSV..."
                )
                batch_callback(batch_results, batch_num)

        elapsed = time.time() - start_time
        self.logger.info(
            f"\nå¹¶è¡Œçˆ¬å–å®Œæˆ!\n"
            f"  - æ€»è€—æ—¶: {elapsed:.1f}s\n"
            f"  - å¹³å‡é€Ÿåº¦: {elapsed/total_items:.2f}s/é¡¹\n"
            f"  - æˆåŠŸ: {success_count}/{total_items} ({success_count/total_items*100:.1f}%)\n"
            f"  - å¤±è´¥: {failed_count}/{total_items} ({failed_count/total_items*100:.1f}%)"
        )

        # ä¿å­˜å¤±è´¥è®°å½•
        if self.failed_items:
            self._save_failed_items()

        return results

    def _save_failed_items(self):
        """ä¿å­˜å¤±è´¥çš„äº§å“è®°å½•"""
        failed_file = Path("data/output/failed_products.json")
        failed_file.parent.mkdir(parents=True, exist_ok=True)

        # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼ŒåŠ è½½å¹¶åˆå¹¶
        existing_failed = []
        if failed_file.exists():
            try:
                with open(failed_file, 'r', encoding='utf-8') as f:
                    existing_failed = json.load(f)
            except:
                pass

        # åˆå¹¶æ–°å¤±è´¥è®°å½•
        all_failed = existing_failed + self.failed_items

        # ä¿å­˜åˆ°æ–‡ä»¶
        with open(failed_file, 'w', encoding='utf-8') as f:
            json.dump(all_failed, f, ensure_ascii=False, indent=2)

        self.logger.info(f"\nâœ— {len(self.failed_items)} ä¸ªå¤±è´¥äº§å“å·²è®°å½•åˆ°: {failed_file}")
        self.logger.info(f"  å¯ä½¿ç”¨ 'uv run python scripts/retry_failed.py' é‡æ–°çˆ¬å–")


def scrape_details_parallel(
    products: List[Dict],
    scrape_detail_func: Callable,
    max_workers: int = 3,
    max_products: int = None,
    retry_times: int = 3,
    request_delay: tuple = (2, 4),
    enable_headless: bool = True,
    batch_size: int = None,
    batch_callback: Callable = None
) -> List[Dict]:
    """
    å¹¶è¡Œçˆ¬å–äº§å“è¯¦æƒ…çš„ä¾¿æ·å‡½æ•°

    Args:
        products: äº§å“åˆ—è¡¨
        scrape_detail_func: è¯¦æƒ…çˆ¬å–å‡½æ•°
        max_workers: æœ€å¤§å¹¶å‘æ•°ï¼Œé»˜è®¤3ï¼ˆé™ä½ä»¥æé«˜ç¨³å®šæ€§ï¼‰
        max_products: æœ€å¤§äº§å“æ•°
        retry_times: å¤±è´¥é‡è¯•æ¬¡æ•°ï¼Œé»˜è®¤3
        request_delay: è¯·æ±‚å»¶è¿ŸèŒƒå›´(ç§’)ï¼Œé»˜è®¤(2, 4)
        enable_headless: æ˜¯å¦å¯ç”¨æ— å¤´æ¨¡å¼
        batch_size: åˆ†æ‰¹å¤§å°ï¼Œæ¯çˆ¬å–Nä¸ªå°±å†™å…¥CSVï¼Œé»˜è®¤Noneï¼ˆä¸åˆ†æ‰¹ï¼‰
        batch_callback: åˆ†æ‰¹å›è°ƒå‡½æ•°ï¼Œæ¥æ”¶(batch_results, batch_num)

    Returns:
        List[Dict]: åŒ…å«è¯¦æƒ…çš„äº§å“åˆ—è¡¨
    """
    scraper = ParallelScraper(
        max_workers=max_workers,
        retry_times=retry_times,
        request_delay=request_delay,
        enable_headless=enable_headless
    )
    return scraper.scrape_items_parallel(
        items=products,
        scrape_func=scrape_detail_func,
        max_items=max_products,
        batch_size=batch_size,
        batch_callback=batch_callback
    )
